import dopamine.discrete_domains.atari_lib
import dopamine.continuous_domains.run_experiment
import dopamine.discrete_domains.gym_lib
import dopamine.jax.agents.sac.sac_agent
import dopamine.jax.agents.dqn.dqn_agent
import dopamine.labs.cale.networks

SACAgent.reward_scale_factor = 0.1
SACAgent.network = @networks.SACCALEConvNetwork
SACAgent.num_layers = 1
SACAgent.hidden_units = 512
SACAgent.gamma = 0.99
SACAgent.update_horizon = 10  # DrQ (instead of 3)
SACAgent.min_replay_history = 1600  # DrQ (instead of 20000)
SACAgent.update_period = 1  # DrQ (rather than 4)
SACAgent.target_update_type = 'soft'
SACAgent.target_smoothing_coefficient = 0.005
SACAgent.target_entropy = None  # Defaults to -num_action_dims/2
SACAgent.optimizer = 'adam'
SACAgent.seed = None  # Seed with the current time
SACAgent.observation_dtype = %sac_agent.IMAGE_DTYPE
SACAgent.stack_size = 4
create_optimizer.learning_rate = 0.0001
create_optimizer.eps = 0.00015

SACCALEAgent.epsilon_train = 0.1
SACCALEAgent.epsilon_eval = 0.05
SACCALEAgent.epsilon_decay_period = 5_000
SACCALEAgent.exploration_strategy = 'eps_greedy'

SACCALEConvNetwork.encoder_name = 'SAC'

ContinuousRunner.create_environment_fn = @atari_lib.create_atari_environment
ContinuousTrainRunner.create_environment_fn = @atari_lib.create_atari_environment
create_atari_environment.game_name = 'Asterix'
create_atari_environment.continuous_action_threshold = 0.5
create_continuous_runner.schedule = 'continuous_train'
create_continuous_agent.agent_name = 'sac_cale_100k'
ContinuousRunner.num_iterations = 10
ContinuousRunner.training_steps = 10_000  # agent steps
ContinuousRunner.max_steps_per_episode = 27_000

atari_lib.create_atari_environment.sticky_actions = False
AtariPreprocessing.terminal_on_life_loss = True

ReplayBuffer.max_capacity = 1_000_000
ReplayBuffer.batch_size = 32
