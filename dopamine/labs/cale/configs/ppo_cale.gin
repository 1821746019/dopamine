import dopamine.discrete_domains.atari_lib
import dopamine.continuous_domains.run_experiment
import dopamine.discrete_domains.gym_lib
import dopamine.jax.agents.ppo.ppo_agent
import dopamine.jax.agents.dqn.dqn_agent
import dopamine.labs.sac_from_pixels.continuous_networks
import dopamine.labs.cale.networks

PPOAgent.network = @networks.PPOCALEConvNetwork
PPOAgent.update_period = 1024  # 8 * 128
PPOAgent.optimizer = 'adam'
PPOAgent.max_gradient_norm = 0.5
create_optimizer.learning_rate = 2.5e-4
create_optimizer.eps = 1e-5
create_optimizer.anneal_learning_rate = True
create_optimizer.anneal_steps = 117_600  # 980 iterations * 3 epochs * 10240 timesteps / 256 batches (TODO(psc): revisit this)
PPOAgent.num_epochs = 3
PPOAgent.batch_size = 256  # 8 * 32
PPOAgent.gamma = 0.99
PPOAgent.lambda_ = 0.95
PPOAgent.epsilon = 0.1
PPOAgent.vf_coefficient = 0.5
PPOAgent.entropy_coefficient = 0.01
PPOAgent.clip_critic_loss = True
PPOAgent.seed = None  # Seed with the current time
PPOAgent.stack_size = 4

PPOCALEAgent.epsilon_train = 0.01
PPOCALEAgent.epsilon_eval = 0.001
PPOCALEAgent.epsilon_decay_period = 250_000
PPOCALEAgent.exploration_strategy = 'standard'

PPOCALEConvNetwork.encoder_name = 'SAC'

ContinuousRunner.create_environment_fn = @atari_lib.create_atari_environment
ContinuousTrainRunner.create_environment_fn = @atari_lib.create_atari_environment
create_atari_environment.game_name = 'Asterix'
#create_atari_environment.use_ppo_preprocessing = True
create_atari_environment.continuous_action_threshold = 0.5
create_continuous_runner.schedule = 'continuous_train'
create_continuous_agent.agent_name = 'ppo_cale'
ContinuousRunner.num_iterations = 200
ContinuousRunner.training_steps = 250_000
ContinuousRunner.max_steps_per_episode = 27_000

ReplayBuffer.max_capacity = 1024
ReplayBuffer.batch_size = 1024
